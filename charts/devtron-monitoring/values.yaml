fluent-bit:
  fullnameOverride: fluent-bit
  enabled: false
  env: []
    # - name: AWS_ACCESS_KEY_ID
    #   value: ""
    # - name: AWS_SECRET_ACCESS_KEY
    #   value: ""
  logLevel: debug
  podAnnotations:
    fluentbit.io/exclude: "true"
  resources: {}
  serviceMonitor:
    enabled: true
  config:
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Kube_Tag_Prefix kube.
          Merge_Log On
          Merge_Log_Key log_processed
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
      [FILTER]
          Name grep
          Match audit.*
          Regex log .*AUDIT_LOG:.*urlPath:\s.[^(health|metrics)].*   
      [FILTER]
          Name grep
          Match nats.*
          Regex log .*NATS_LOG.*
      [FILTER]
          Name grep
          Match error.*
          Regex log .*(DEVTRON_PANIC_RECOVER|nats: found panic error).*      
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker, cri
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag kube.<namespace_name>.<pod_name>.<container_name>
          Mem_Buf_Limit 50MB
          Skip_Long_Lines On
          DB /var/log/flb_kube.db
          Refresh_Interval  60
      [INPUT]
          Name tail
          Path /var/log/containers/*orchestrator*.log
          multiline.parser docker, cri
          Tag audit.*
          Mem_Buf_Limit 150MB
          Skip_Long_Lines On
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval  30
      [INPUT]
          Name systemd
          Tag host.*
          Systemd_Filter _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail On
      [INPUT]
          Name tail
          Path /var/log/containers/*_devtroncd_*.log , /var/log/containers/*_devtron-ci_*.log 
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker,cri 
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag error.<namespace_name>.<pod_name>.<container_name>
          Mem_buf_Limit 50MB
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval 30
      [INPUT]
          Name tail
          Path /var/log/containers/*_devtroncd_*.log , /var/log/containers/*_devtron-ci_*.log 
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker,cri 
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag nats.<namespace_name>.<pod_name>.<container_name>
          Mem_buf_Limit 50MB
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval 30
    outputs: |
      [OUTPUT]
          Name s3
          Match kube.*
          region <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket <bucket_name>
          s3_key_format /$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S_$TAG[2].log
          # s3_key_format /$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR 
      [OUTPUT]
          Name s3
          Match audit.*
          region <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket <bucket_name>
          s3_key_format /audit_logs/orchestrator/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR    
      [OUTPUT]
          Name s3
          Match nats.*
          region  <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket  <bucket_name>
          s3_key_format /nats_pub_sub/$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR   
      [OUTPUT]
          Name s3
          Match error.*
          region  <bucket_region>
          bucket  <bucket_name>
          s3_key_format /panic/$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR 
jaeger-all-in-one-opentelemetry:
  Deployment:
    Replicas: 1
    image: otel/opentelemetry-collector:0.83.0
  enabled: false
  ingress:
    className: nginx
    enabled: true
    hosts:
      - host: jaeger-<testing>.domain.com
        pathType: Prefix
        paths: /
  jaeger:
    env:
      - name: SPAN_STORAGE_TYPE
        value: badger
      - name: COLLECTOR_OTLP_ENABLED
        value: "true"
      - name: BADGER_EPHEMERAL
        value: "false"
      - name: BADGER_DIRECTORY_VALUE
        value: /badger/data
      - name: BADGER_DIRECTORY_KEY
        value: /badger/key
      - name: BADGER_SPAN_STORE_TTL
        value: 168h
    image: jaegertracing/all-in-one:1.48
    replicas: 1
    storage: 10Gi
  optl_collector_config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    processors:
      batch:
      memory_limiter:
        # 80% of maximum memory up to 2G
        limit_mib: 1500
        # 25% of limit up to 2G
        spike_limit_mib: 512
        check_interval: 5s
    extensions:
      zpages: {}
      memory_ballast:
        # Memory Ballast size should be max 1/3 to 1/2 of memory.
        size_mib: 683
    exporters:
      logging:
        loglevel: info
      jaeger:
        endpoint: jaeger-headless.opentelemetry:14250
        tls:
          insecure: true
    service:
      extensions: [zpages, memory_ballast]
      pipelines:
        traces/1:
          receivers: [otlp]
          #processors: [memory_limiter, batch]
          exporters: [logging, jaeger]
k8s-event-logger:
  enabled: false
metrics-server:
  enabled: false
  replicas: 1
  resources: {}
prometheus-blackbox-exporter:
  enabled: false
uptime-kuma:
  enabled: false
  ingress:
    className: nginx
    enabled: false
    hosts:
      - host: uptime-<testing>.domain.com
        paths:
          - path: /
            pathType: ImplementationSpecific
  podEnv:
    - name: UPTIME_KUMA_PORT
      value: "3001"
  resources: {}
  serviceMonitor:
    enabled: true
  volume:
    accessMode: ReadWriteOnce
    enabled: false
    size: 1Gi
vector:
  customConfig:
    data_dir: /var/lib/vector
    sinks:
      my_sink_id:
        batch:
          timeout_secs: 600
        bucket: <bucket_name>
        compression: none
        encoding:
          codec: text
        filename_append_uuid: false
        filename_time_format: "%H_%M_%S"
        inputs:
          - my_remap_id_audit
        key_prefix: '{{ print "audit-log/devtroncd/{{ container_name }}/%Y-%m-%d/" }}'
        region: <region_name>
        type: aws_s3
      my_sink_id_audit:
        batch:
          timeout_secs: 600
        bucket: <bucket_name>
        compression: none
        encoding:
          codec: text
        filename_append_uuid: false
        filename_time_format: "%H_%M_%S"
        inputs:
          - my_remap_id
        key_prefix: '{{ print "vector-log-2/devtroncd/{{ container_name }}/%Y-%m-%d/" }}'
        region: <region_name>
        type: aws_s3
    sources:
      kube_log:
        auto_partial_merge: true
        extra_namespace_label_selector: kubernetes.io/metadata.name in (devtroncd)
        timezone: Asia/Kolkata
        type: kubernetes_logs
    transforms:
      filter_audit_logs:
        condition: string!(.message) != null && contains(string!(.message), "AUDIT_LOG")
        inputs:
          - kube_log
        type: filter
      my_remap_id:
        inputs:
          - kube_log
        source: |-
          .test = del(.)
          .container_name = del(.test.kubernetes.container_name)
          .message = del(.test.message)
          .timestamp = del(.test.timestamp)
          del(.test)
        type: remap
      my_remap_id_audit:
        inputs:
          - filter_audit_logs
        source: |-
          .test = del(.)
          .container_name = del(.test.kubernetes.container_name)
          .message = del(.test.message)
          .timestamp = del(.test.timestamp)
          del(.test)
        type: remap
  enabled: false
  resources: {}
  role: Agent
  service:
    ports:
      - name: http
        port: 9090
victoriametrics:
  enabled: true
  alertmanager:
    config:
      receivers:
        - name: "null"
        - discord_configs:
            - send_resolved: true
              title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
              webhook_url: ""
          name: dev-team-watchdog
        - discord_configs:
            - send_resolved: true
              title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
              webhook_url: ""
          name: dev-team-discord
        - discord_configs:
            - send_resolved: true
              title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
              webhook_url: ""
          name: discord
      route:
        group_by:
          - alertname
          - cluster
          - service
        group_interval: 5m
        group_wait: 30s
        receiver: discord
        repeat_interval: 12h
        routes:
          - matchers:
              - alertname =~ "InfoInhibitor|Watchdog|NginxHighHttp403ErrorRate.*"
            receiver: null
          - matchers:
              - namespace !~ "devtroncd|devtron-ci|devtroncd-cd|opentelemetry|monitoring"
            receiver: null
          - match:
              namespace: devtron-demo
            receiver: null
          - match:
              namespace: demo-env
            receiver: null
          - match:
              namespace: devtroncd
            receiver: discord
          - match:
              alertname: Watchdog
            receiver: dev-team-watchdog
          - match:
              team: devtron
            receiver: dev-team-discord
          - match:
              alertname: NodeCpuSaturation
            receiver: null
          - match:
              alertname: KubeCPUOvercommit
            receiver: null
          - match:
              alertname: CPUThrottlingHigh
            receiver: null
          - match:
              alertname: KubeContainerWaiting
            receiver: null
          - match:
              alertname: PrometheusRuleFailures
            receiver: null
          - match:
              alertname: KubeSchedulerDown
            receiver: null
          - match:
              alertname: KubeControllerManagerDown
            receiver: null
          - match:
              alertname: KubeClientCertificateExpiration
            receiver: null
          - match:
              alertname: NodeFilesystemSpaceFillingUp
            receiver: null
          - match:
              alertname: KubeProxyDown
            receiver: null
          - match:
              alertname: NoOutputBytesProcessed
            receiver: null
          - match:
              alertname: TargetDown
              job: kubelet
              service: monitoring-prometheus-oper-kubelet
            receiver: null
          - match:
              job: kube-proxy
            receiver: null
          - match:
              namespace: devtron-dev
            receiver: null
          - match:
              container: metrics-server
            receiver: null
          - match:
              alertname: ArgoCDAppDegraded
            receiver: null
          - match:
              alertname: ArgoCDAppMissing
            receiver: null
          - match:
              alertname: ArgoCDAppUnknown
            receiver: null
          - match:
              container: fluent-bit
            receiver: null
          - match:
              alertname: KubeJobFailed
            receiver: null
          - continue: true
            match: null
            receiver: discord
    enabled: true
  crds:
    enabled: true
  defaultDashboards:
    defaultTimezone: ist
    enabled: true
  fullnameOverride: victoria-metrics
  grafana:
    global:
      imageRegistry: mirror.gcr.io
    adminPassword: change-me
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - disableDeletion: false
            editable: true
            folder: ""
            name: default
            options:
              path: /var/lib/grafana/dashboards/default
            orgId: 1
            type: file
    dashboards:  {}
    defaultDashboardsTimezone: ist
    enabled: true
    forceDeployDatasource: false
    grafana.ini:
      auth.anonymous:
        enabled: true
        org_name: devtron-metrics-view
        org_role: Viewer
      repository: grafana/grafana
      security:
        allow_embedding: true
      server:
        root_url: https://<testing>.domain.com/grafana
        serve_from_sub_path: true
      tag: 9.4.17
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
        nginx.ingress.kubernetes.io/rewrite-target: /grafana/$2
        nginx.ingress.kubernetes.io/ssl-redirect: "false"
      enabled: true
      hosts:
        - <testing>.domain.com
      ingressClassName: nginx-new
      labels: {}
      path: /grafana(/|$)(.*)
      pathType: Prefix
      tls: []
    persistence:
      accessModes:
        - ReadWriteOnce
      enabled: true
      size: 5Gi
      type: pvc
    sidecar:
      dashboards:
        additionalDashboardAnnotations: {}
        additionalDashboardLabels: {}
        enabled: false
        multicluster: false
      datasources:
        createVMReplicasDatasources: false
        enabled: true
        initDatasources: true
        jsonData: {}
    vmServiceScrape:
      enabled: true
  kube-state-metrics:
    enabled: true
    metricLabelsAllowlist:
      - pods=[*]
      - nodes=[*]
  prometheus-node-exporter:
    enabled: true
  victoria-metrics-operator:
    operator:
      disable_prometheus_converter: false
      prometheus_converter_add_argocd_ignore_annotations: true
  vmagent:
    enabled: true
    ingress:
      enabled: true
      hosts:
        - vmagent-stage-<testing>.domain.com
      ingressClassName: nginx-new
    spec:
      externalLabels:
        cluster: cluster-name
        enterprise: testing
      extraArgs:
        promscrape.maxScrapeSize: "33554432"
        promscrape.suppressScrapeErrors: "true"
        promscrape.suppressScrapeErrorsDelay: 30s
      scrapeInterval: 20s
  vmalert:
    enabled: true
    ingress:
      enabled: true
      hosts:
        - vmalert-<testing>.domain.com
      ingressClassName: nginx
    spec:
      extraArgs:
        external.url: https://vmalert-<testing>.domain.com
  vmsingle:
    enabled: true
    ingress:
      enabled: true
      hosts:
        - vmsingle-<testing>.domain.com
      ingressClassName: nginx
    spec:
      retentionPeriod: 5d
